# ğŸ¯ Fine-Tuning em LLMs para Text-to-SQL e AvaliaÃ§Ã£o no MMLU

Este repositÃ³rio contÃ©m um pipeline completo para avaliar o impacto do **fine-tuning via LoRA** em Modelos de Linguagem de Grande Porte (LLMs), aplicado na tarefa de **Text-to-SQL** (Spider) e na avaliaÃ§Ã£o de generalizaÃ§Ã£o no benchmark **MMLU (Massive Multitask Language Understanding)**.

---

## ğŸ“‘ DescriÃ§Ã£o das Fases do Projeto

### ğŸ”¹ **Fase 1: Estabelecimento do Baseline**
- **1.1 Prompt Engineering:**  
CriaÃ§Ã£o de um prompt few-shot contendo 3 exemplos representativos. Este template Ã© fixo para todas as avaliaÃ§Ãµes de baseline.
  
- **1.2 AvaliaÃ§Ã£o do Modelo Base:**  
SubmissÃ£o do modelo **sem fine-tuning** ao Spider (dev split) utilizando o prompt elaborado.

- **1.3 Coleta de Dados:**  
Armazenamento das queries SQL geradas e cÃ¡lculo da taxa de sucesso/falha.

---

### ğŸ”¸ **Fase 2: ExecuÃ§Ã£o do Fine-Tuning**
---

### ğŸ§  **Fase 3: AvaliaÃ§Ã£o Customizada na Tarefa-Alvo**
- **3.1 ImplementaÃ§Ã£o de MÃ©trica Customizada:**  
Desenvolvimento da mÃ©trica **Execution Accuracy** no framework **DeepEval**, com as seguintes etapas:
  - ExecuÃ§Ã£o da query SQL gerada (`actual_output`) e da ground truth (`expected_output`) em um banco SQLite com os dados do Spider.
  - ComparaÃ§Ã£o dos resultados de forma **order-insensitive**.
  - Retorno de **1.0 (sucesso)** ou **0.0 (falha)**.

- **3.2 AvaliaÃ§Ã£o Automatizada:**  
IntegraÃ§Ã£o da mÃ©trica em testes via **Pytest**, rodando avaliaÃ§Ãµes completas no Spider dev split.

---

### ğŸ“‰ **Fase 4: AnÃ¡lise de RegressÃ£o de Capacidade (GeneralizaÃ§Ã£o)**
- **4.1 AvaliaÃ§Ã£o no MMLU:**  
ExecuÃ§Ã£o de 150 questÃµes do benchmark MMLU em modo **4-shot**, tanto para o modelo base quanto para os modelos fine-tuned.

- **4.2 CÃ¡lculo de AcurÃ¡cia:**  
MediÃ§Ã£o da acurÃ¡cia como proporÃ§Ã£o de respostas corretas.

- **4.3 AnÃ¡lise de RegressÃ£o:**  
CÃ¡lculo da **variaÃ§Ã£o percentual de acurÃ¡cia** entre o modelo base e os modelos fine-tuned:
  - AnÃ¡lise global.
  - AnÃ¡lise por categoria: **STEM**, **Humanidades**, **CiÃªncias Sociais**.

---

## ğŸš€ Como Executar os Experimentos

### âœ… **Requisitos**
- Conta no **Google Colab**.
- **GPU NVIDIA A100 (40 GB)** alocada no Colab.
- DependÃªncias listadas em `requirements.txt`.

### ğŸ”¥ **Passos**
1. Clone ou faÃ§a fork deste repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/seu-repositorio.git
   cd seu-repositorio```

2. No Google Colab:
   * Abra cada notebook na seguinte ordem:
     * `fase-01.ipynb`
     * `fase-02.ipynb`
     * `fase-03.ipynb`
     * `fase-04.ipynb`
   * Verifique se a GPU A100 (40 GB) estÃ¡ ativa:
     **Ambiente de execuÃ§Ã£o â†’ Alterar tipo de hardware â†’ GPU â†’ A100**

3. Execute cada notebook sequencialmente para:

   * Criar o baseline.
   * Realizar o fine-tuning com diferentes hiperparÃ¢metros.
   * Avaliar na tarefa Text-to-SQL com a mÃ©trica customizada.
   * Avaliar no benchmark MMLU para anÃ¡lise de regressÃ£o.
---

## ğŸ“‚ Estrutura dos Arquivos

```
â”œâ”€â”€ custom_metrics/         # MÃ©tricas customizadas (Execution Accuracy)
â”œâ”€â”€ models/                 # Adaptadores jÃ¡ treinados para nÃ£o gastar muito com GPU
â”œâ”€â”€ scripts/                # Scripts auxiliares e anÃ¡lise
â”œâ”€â”€ fase-01.ipynb           # Baseline Text-to-SQL
â”œâ”€â”€ fase-02.ipynb           # Fine-tuning com LoRA
â”œâ”€â”€ fase-03.ipynb           # AvaliaÃ§Ã£o com mÃ©tricas customizadas
â”œâ”€â”€ fase-04.ipynb           # AvaliaÃ§Ã£o no MMLU
â”œâ”€â”€ requirements.txt        # DependÃªncias
â””â”€â”€ README.md               # Este arquivo
```
